---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Riccardo Fusaroli"
date: "October 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 3 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of quantitative descriptors of voice in schizophrenia, focusing on pitch.
In the course of this assignment we will use them to try to automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.


```{r}
#------ Libraries ------
library(readr)
library(crqa)
library(stringr)
library(lme4)
library(lmerTest)
library(stats)
library(ggplot2)
library(caret)
library(e1071)
library(pROC) 
library(data.table)
library(MuMIn)

#------ WD -------
wd = getwd()
setwd(wd)

wd

```


### Question 1: Can you diagnose schizophrenia from pitch range only? If so, how well?

Build a logistic regression to see whether you can diagnose schizophrenia from pitch range only.

Calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve) on a logistic regression using the full dataset. Don't forget the random effects!

Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures.

N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one datase, so to calculate overall performance.
N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?

```{r}
#------ Logistic regression ------
#Add range
data$range = data$rangeU - data$rangeL

#Make logistic regression
qplot(sample = data$range)

data$Diagnosis = as.factor(data$Diagnosis)

m_logis = glmer(Diagnosis ~ range + (1|Subject), family = binomial, data)
summary(m_logis)

#------ Test performance of regression ------
data$predicted = predict(m_logis, type = 'response')
data$predicted = ifelse(data$predicted > 0.5, 1, 0)

cm = confusionMatrix(data = data$predicted, reference = data$Diagnosis)

cm$overall[1]


rocCurve = roc(response = data$Diagnosis, predictor = data$predicted)
auc(rocCurve)
ci (rocCurve) 
plot(rocCurve, legacy.axes = TRUE) 

#------ Cross validation ------
#------ Fold data ------
#Create fold with unique SUBJ
folds=createFolds(unique(data$Subject), k = 4)
data$SubjectN=as.numeric(as.factor(data$Subject))


#--------Create null list for results --------
results = data.frame()


#----------------Loop----------------

for (k in folds){
  #------ Split into training and test data ------ 
  #Create training dataset, data not in fold k
  data_train=subset(data,!(SubjectN %in% k))
  #Create test dataset, data in fold k
  data_test=subset(data,SubjectN %in% k)
  
  #------ train model - apply model to data_train ------
  logis_model = glmer(Diagnosis ~ range + (1|Study), family = binomial, data_train, control = glmerControl(calc.derivs = FALSE))
  
  predict_train = predict(logis_model, data_train, type = 'response') #ask about response
  predict_train = ifelse(predict_train > 0.5, 1, 0)
  
  cm_train = confusionMatrix(data = predict_train, reference = data_train$Diagnosis)
  
  roc_train = roc(response = data_train$Diagnosis, predictor = predict_train)
  auc_train = auc(roc_train)
  
  #------ test the model - test model on data_test (last quarter) ------
  #Make predictions based on modeVIS
  predict_test=predict(logis_model, data_test, type = 'response', allow.new.levels = TRUE)
  predict_test = ifelse(predict_test > 0.5, 1, 0)
  
  cm_test = confusionMatrix(data = predict_test, reference = data_test$Diagnosis)
  
  roc_test = roc(response = data_test$Diagnosis, predictor = predict_test)
  auc_test = auc(roc_test)
  
  #------ save the performance ------ 
  
  
  one_row = data.frame(acc_train = cm_train$overall[1], acc_test = cm_test$overall[1], sens_train = cm_train$byClass[1], sens_test = cm_test$byClass[1], spec_train = cm_train$byClass[2], spec_test = cm_test$byClass[2], auc_train = auc_train, auc_test = auc_test,  npv_train = cm_train$byClass[3], npv_test = cm_test$byClass[3], ppv_train = cm_train$byClass[4], ppv_test = cm_test$byClass[4])
  
  
  results = rbind(results, one_row)
    }




```


### Question 2 - Which single acoustic predictor is the best predictor of diagnosis?
The single best predictors is SD if we look at AUC test
```{r}
#--------Create null list for results --------
m_comp = data.frame()

features = list("range","sd", "IQR", "nsyll", "npause", "RR", "DET", "NRLINE", "maxL", "L", "ENTR", "rENTR", "LAM", "TT")

data_BackUp = data
#----------------Loop----------------
for (f in features){
  #Empty data frame
  feat_results = data.frame()

  for (k in folds){
    #Change the name of the ith variable in f to "f", e.g. range --> f
    setnames(data, f, "f")
    #------ Split into training and test data ------ 
    #Create training dataset, data not in fold k
    data_train=subset(data,!(SubjectN %in% k))
    #Create test dataset, data in fold k
    data_test=subset(data,SubjectN %in% k)
    
    
    
    #------ train model - apply model to data_train ------
    #Model uses the variable renamed to f above
    logis_model = glmer(Diagnosis ~ f + (1|Study), family = binomial, data_train, control = glmerControl(calc.derivs = FALSE))
    
    #Get predicted value for training data, and round to 0 or 1
    predict_train = predict(logis_model, data_train, type = 'response')
    predict_train = ifelse(predict_train > 0.5, 1, 0)
    
    #Make confusion matrix and other stats
    cm_train = confusionMatrix(data = predict_train, reference = data_train$Diagnosis)
    
    roc_train = roc(response = data_train$Diagnosis, predictor = predict_train)
    auc_train = auc(roc_train)
    
    #------ test the model - test model on data_test (last quarter) ------
    #Make predictions based on modeVIS
    predict_test=predict(logis_model, data_test, type = 'response', allow.new.levels = TRUE)
    predict_test = ifelse(predict_test > 0.5, 1, 0)
    
    cm_test = confusionMatrix(data = predict_test, reference = data_test$Diagnosis)
    
    roc_test = roc(response = data_test$Diagnosis, predictor = predict_test)
    auc_test = auc(roc_test)
    
    #------ save the performance ------ 
    #Save all the variables in one row
    one_row = data.frame(acc_train = cm_train$overall[1], acc_test = cm_test$overall[1], sens_train = cm_train$byClass[1], sens_test = cm_test$byClass[1], spec_train = cm_train$byClass[2], spec_test = cm_test$byClass[2], auc_train = auc_train, auc_test = auc_test,  npv_train = cm_train$byClass[3], npv_test = cm_test$byClass[3], ppv_train = cm_train$byClass[4], ppv_test = cm_test$byClass[4])
    
    #Add the row to the data set
    feat_results = rbind(feat_results, one_row)
    
    #rename the variable back to "f", e.g. f --> range
    setnames(data, "f", f)
    }
  
  #Take mean of the test stats
  feat_results2 = data.frame(feature = f,acc_train = mean(feat_results$acc_train), acc_test = mean(feat_results$acc_test), sens_train = mean(feat_results$sens_train), sens_test =  mean(feat_results$sens_test), spec_train = mean(feat_results$spec_train), spec_test =  mean(feat_results$spec_test), auc_train =  mean(feat_results$auc_train), auc_test =  mean(feat_results$auc_test),  npv_train =  mean(feat_results$npv_train), npv_test =  mean(feat_results$npv_test), ppv_train =  mean(feat_results$ppv_train), ppv_test =  mean(feat_results$ppv_test))
  
  m_comp = rbind(m_comp, feat_results2)
  }
  
```



### Question 3 - Which combination of acoustic predictors is best for diagnosing schizophrenia?

Now it's time to go wild! Use all (voice-related) variables and interactions you can think of. Compare models and select the best performing model you can find.

Remember:
- Out-of-sample error crucial to build the best model!
- After choosing the model, send Celine and Riccardo the code of your model
```{r}
#------ Cross validation ------

#Model: Diagnosis ~ DET + TT + rENTR + sd +(1|Study)
#RR: recurrence points/total amount of points, how black the plot is
#NRLINE: number of lines, i.e. how often a series is repeated
#DET: mÃ¥l for hvor ofte requrrence pointsne er en del af en gentaget sekvens, expression of series of pitches being repeated --> catatonic
#LAM: measure of how many recurrence points are part of veritcal lines. expression of same pitch being repeated
#TT: measures lenght length of the vertical lines (LAM is the percentage of dots on vertical lines)
#ENTR: Compleksiteten af systemet af gentagne frekevenser
#rENTR: normaliseret entropi



#--------Create null list for results --------
result.2.3 = data.frame()


#----------------Loop----------------

for (k in folds){
  #------ Split into training and test data ------ 
  #Create training dataset, data not in fold k
  data_train=subset(data,!(SubjectN %in% k))
  #Create test dataset, data in fold k
  data_test=subset(data,SubjectN %in% k)
  
  #------ train model - apply model to data_train ------
  m_ny = glmer(Diagnosis ~ DET + TT + rENTR + sd + (1|Study), family = binomial, data_train, control = glmerControl(calc.derivs = FALSE))
  
  predict_train = predict(m_sdrr, data_train, type = 'response') #ask about response
  predict_train = ifelse(predict_train > 0.5, 1, 0)
  
  cm_train = confusionMatrix(data = predict_train, reference = data_train$Diagnosis)
  
  roc_train = roc(response = data_train$Diagnosis, predictor = predict_train)
  auc_train = auc(roc_train)
  
  #------ test the model - test model on data_test (last quarter) ------
  #Make predictions based on modeVIS
  predict_test=predict(m_sdrr, data_test, type = 'response', allow.new.levels = TRUE)
  predict_test = ifelse(predict_test > 0.5, 1, 0)
  
  cm_test = confusionMatrix(data = predict_test, reference = data_test$Diagnosis)
  
  roc_test = roc(response = data_test$Diagnosis, predictor = predict_test)
  auc_test = auc(roc_test)
  
  #------ save the performance ------ 
  
  
  one_row = data.frame(acc_train = cm_train$overall[1], acc_test = cm_test$overall[1], sens_train = cm_train$byClass[1], sens_test = cm_test$byClass[1], spec_train = cm_train$byClass[2], spec_test = cm_test$byClass[2], auc_train = auc_train, auc_test = auc_test,  npv_train = cm_train$byClass[3], npv_test = cm_test$byClass[3], ppv_train = cm_train$byClass[4], ppv_test = cm_test$byClass[4])
  
  
  results.2.3 = rbind(results, one_row)
}

#using best AUC predictors, AIC:1781.05 
m_ssrr = glmer(Diagnosis ~ sd + RR + (1|Study), family = binomial, data, control = glmerControl(calc.derivs = FALSE))

#Thinking theoretically about RQA predictors. Worse AIC than the above model: 1802.68
m_roland = glmer(Diagnosis ~ TT + LAM + rENTR + (1|Study), family = binomial, data, control = glmerControl(calc.derivs = FALSE))

#add more predictors. still worse AIC: 1797.02
m_ny = glmer(Diagnosis ~ DET + TT + rENTR + sd + (1|Study), family = binomial, data, control = glmerControl(calc.derivs = FALSE))

#r.squaredGLMM(m_roland)
AIC(m_ssrr)
#BIC(m_logis)
```


### Question 4: Properly report the results

METHODS SECTION: how did you analyse the data? That is, how did you extract the data, designed the models and compared their performance?

RESULTS SECTION: can you diagnose schizophrenia based on voice? which features are used? Comment on the difference between the different performance measures.

### Bonus question 5

You have some additional bonus data involving speech rate, pauses, etc. Include them in your analysis. Do they improve classification?

### Bonus question 6

Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them.

